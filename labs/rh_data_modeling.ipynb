{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "@author: Abillelatus (Ryan Herrin)\n",
    "\n",
    "Code for testing and modeling Powershell Obscuration classification using a provided \n",
    "data set with predetermined features. PCA may be the preferred method for the unsupervised approach as \n",
    "the data provided is not labled. \n",
    "\n",
    "Current Models do not include the \"sha1\" and \"fpath\" columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Global variables\n",
    "# Define Global\n",
    "\n",
    "#original_data_path = \"../dataset/all.csv\" # this file is originally zipped\n",
    "original_data_path = \"../dataset/all_with_labels.csv\" # With Labels \n",
    "\n",
    "# Some columns are empty for future use. Set to False to not include them\n",
    "include_unused_columns = False \n",
    "\n",
    "# Define unused columns here\n",
    "unused_columns = ['vt_harmless', 'vt_undetected', 'vt_malicious', 'vt_suspicious',\n",
    "\t\t\t\t  'avclass_name', 'obf_name']\n",
    "\n",
    "# Remove the string columns sha1 and fpath\n",
    "include_sha1_fpath = False\n",
    "\n",
    "# Remove them from the list if set to False \n",
    "if include_sha1_fpath == False:\n",
    "\tunused_columns.append('sha1')\n",
    "\tunused_columns.append('fpath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create a function that imports the dataset and returns a workable formatted version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workable_data(file_path, lst_unused_columns):\n",
    "\t'''import the dataset and format the data into a pandas dataframe.\n",
    "\t+ Read in CSV as dataframe\n",
    "\t+ Remove extra header if there is one \n",
    "\t+ Remove unused columns if set \n",
    "\t+ Typecast DataTypes to string and integers\n",
    "\t+ Returns Pandas DataFrame object'''\n",
    "\ttry:\n",
    "\t\tpd_df = pd.read_csv(file_path) # Read in CSV as dataframe\n",
    "\texcept FileNotFoundError():\n",
    "\t\tprint(\"Could not find data file. Make sure the file is unzipped...\")\n",
    "\t\tsys.exit(1)\n",
    "\t\t\n",
    "\t# Remove Duplicates\n",
    "\tpd_df.drop_duplicates('sha1', inplace=True)\n",
    "\t\t\n",
    "\t# Check to see if duplicate header is present. Remove if there is \n",
    "\tif pd_df.at[0, 'sha1'] == 'sha1':\n",
    "\t\tpd_df = pd_df.iloc[1:] # .iloc[] integer-loc based indexing for selecting by position \n",
    "\t\t\n",
    "\t# Remove unused columns if global var \"include_unused_columns\" is set to False\n",
    "\tif include_unused_columns == False:\n",
    "\t\tpd_df = pd_df.drop(columns=lst_unused_columns)\n",
    "\n",
    "\t# Convert all to float \n",
    "\tfor col in pd_df.columns.tolist():\n",
    "\t\tpd_df[col] = pd_df[col].astype(float)\n",
    "\t\n",
    "\treturn(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create reusable function to remove rows from dataframe that are equal to or lower than the value provided. Initially used to remove scripts that\n",
    "have no length to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_df_rows(data_as_df, char_limit):\n",
    "    '''Function to remove rows based on the char_limit. char_limit is depicted by\n",
    "    the doc_char_count feature. \n",
    "    @Params:\n",
    "    - data_as_df (Pandas DataFrame): Data set\n",
    "    - char_limit (int): Will remove all rows less than or equal to this value\n",
    "    '''\n",
    "    working_data = data_as_df # Copy to return\n",
    "    indx_to_remove = [] # List of index values to remove from DF\n",
    "    # Find Index's of rows that match the use case \n",
    "    for indx in working_data.index.values:\n",
    "        if float(working_data.at[indx, 'doc_char_count']) <= char_limit:\n",
    "            indx_to_remove.append(indx)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print('{} row(s) removed that had a document char lenth of {} or less.'.format(\n",
    "        len(indx_to_remove), char_limit\n",
    "    ))\n",
    "\n",
    "    return(working_data.drop(indx_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Initial dataframes to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 row(s) removed that had a document char lenth of 0 or less.\n"
     ]
    }
   ],
   "source": [
    "data_df = create_workable_data(original_data_path, unused_columns) # Create a Data Frame from the csv \n",
    "data_df = remove_df_rows(data_df, 0) # Remove rest of the empty rows \n",
    "\n",
    "'''\n",
    "Note: data_df will be the original data and should not be modified as it will be used\n",
    "    to compare to delta DataFrames.\n",
    "'''\n",
    "# Create Delta Data Frame. This will be used for modifications\n",
    "delta_data_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating more of an investigative function that will show how much of the data is represented depending on a user selected interval. Say you want to know the percentage of script char counts with intervals of 500 to 10,000. It will print out the percentage of 1-500, 501-1000,... 10000-above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char Count\t|\tNum of Scripts\t|\tPercent of Data\n",
      "----------\t|\t--------------\t|\t---------------\n",
      "1-5000\t\t\t153677\t\t0.7522860779322499\n",
      "5001-10000\t\t\t24293\t\t0.11892010965341687\n",
      "10001-15000\t\t\t10294\t\t0.05039161934599569\n",
      "15001-20000\t\t\t4695\t\t0.022983160368122185\n",
      "20001-25000\t\t\t2436\t\t0.011924809085568828\n",
      "25001-30000\t\t\t1825\t\t0.008933816330526728\n",
      "30001-35000\t\t\t1236\t\t0.0060505188956334445\n",
      "35001-40000\t\t\t779\t\t0.003813393381633053\n",
      "40001-45000\t\t\t629\t\t0.00307910710789113\n",
      "45001-50000\t\t\t427\t\t0.002090268259252007\n",
      "above 50000\t\t\t3989\t\t0.0195271196397102\n"
     ]
    }
   ],
   "source": [
    "# Show distribution in text first because the current data does not translate over to histogram  \n",
    "def show_freq_list(data, interval, max):\n",
    "    \"\"\"Show a list of amount of scripts that fall between the frequency intervals\n",
    "    from 0 to the user defined interval. The max set the top limit, as in anything \n",
    "    above the max value will be included in the '<Max> and Above'.\n",
    "    \"\"\"\n",
    "    max_limit = max\n",
    "    low_cnt = 0\n",
    "    high_cnt = interval\n",
    "    name_list = []\n",
    "    count_list = []\n",
    "    percent_list = [] # Percentage of scripts covered\n",
    "    \n",
    "    print(\"Char Count\\t|\\tNum of Scripts\\t|\\tPercent of Data\")\n",
    "    print(\"----------\\t|\\t--------------\\t|\\t---------------\")\n",
    "\n",
    "    while low_cnt < max_limit:\n",
    "        low_cnt = low_cnt + 1\n",
    "        name_list.append(str(low_cnt) + '-' + str(high_cnt))\n",
    "\n",
    "        num_of_instances = len(\n",
    "            data_df[(data_df['doc_char_count'] >= low_cnt) & (data_df['doc_char_count'] <= high_cnt)]\n",
    "        )\n",
    "\n",
    "        count_list.append(num_of_instances)\n",
    "        percent_list.append(num_of_instances / data.shape[0])\n",
    "\n",
    "        print('{}\\t\\t\\t{}\\t\\t{}'.format(name_list[-1], count_list[-1], percent_list[-1]))\n",
    "\n",
    "        # Update values\n",
    "        low_cnt = low_cnt + interval -1\n",
    "        high_cnt = high_cnt + interval\n",
    "\n",
    "    print('above {}\\t\\t\\t{}\\t\\t{}'.format(\n",
    "        max_limit, \n",
    "        data.shape[0] - sum(count_list), \n",
    "        (data.shape[0] - sum(count_list))/data.shape[0])\n",
    "    )\n",
    "    #TODO: Create aggregate. Ex. 1-100: %   |   101-max: %   | max-above: %\n",
    "\n",
    "show_freq_list(delta_data_df, 5000, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_obf</th>\n",
       "      <th>char_hash_count</th>\n",
       "      <th>char_paren_count</th>\n",
       "      <th>char_brack_count</th>\n",
       "      <th>char_brace_count</th>\n",
       "      <th>char_bkslash_count</th>\n",
       "      <th>char_fwslash_count</th>\n",
       "      <th>char_dollar_count</th>\n",
       "      <th>char_squote_count</th>\n",
       "      <th>char_dquote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>doc_keyword_return_word_count</th>\n",
       "      <th>doc_keyword_static_word_count</th>\n",
       "      <th>doc_keyword_switch_word_count</th>\n",
       "      <th>doc_keyword_throw_word_count</th>\n",
       "      <th>doc_keyword_trap_word_count</th>\n",
       "      <th>doc_keyword_try_word_count</th>\n",
       "      <th>doc_keyword_until_word_count</th>\n",
       "      <th>doc_keyword_using_word_count</th>\n",
       "      <th>doc_keyword_var_word_count</th>\n",
       "      <th>doc_keyword_while_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>204280.000000</td>\n",
       "      <td>2.042800e+05</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>2.042800e+05</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>2.042800e+05</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>2.042800e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "      <td>204280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.019703</td>\n",
       "      <td>6.025353e+01</td>\n",
       "      <td>74.440043</td>\n",
       "      <td>5.056465e+01</td>\n",
       "      <td>94.351963</td>\n",
       "      <td>11.562571</td>\n",
       "      <td>7.086974e+01</td>\n",
       "      <td>112.866277</td>\n",
       "      <td>30.245565</td>\n",
       "      <td>6.672977e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.277697</td>\n",
       "      <td>0.077521</td>\n",
       "      <td>0.206599</td>\n",
       "      <td>0.432113</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.526997</td>\n",
       "      <td>0.041428</td>\n",
       "      <td>0.272983</td>\n",
       "      <td>0.256893</td>\n",
       "      <td>0.153363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.224827</td>\n",
       "      <td>3.665022e+03</td>\n",
       "      <td>576.111577</td>\n",
       "      <td>4.289082e+03</td>\n",
       "      <td>3400.263950</td>\n",
       "      <td>630.926984</td>\n",
       "      <td>3.748793e+03</td>\n",
       "      <td>1789.083560</td>\n",
       "      <td>291.065866</td>\n",
       "      <td>5.685146e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>8.710415</td>\n",
       "      <td>2.402772</td>\n",
       "      <td>1.489893</td>\n",
       "      <td>6.803669</td>\n",
       "      <td>0.278518</td>\n",
       "      <td>6.689347</td>\n",
       "      <td>0.461789</td>\n",
       "      <td>2.055392</td>\n",
       "      <td>2.049805</td>\n",
       "      <td>0.928370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.280462e+06</td>\n",
       "      <td>165592.000000</td>\n",
       "      <td>1.918417e+06</td>\n",
       "      <td>580658.000000</td>\n",
       "      <td>279010.000000</td>\n",
       "      <td>1.104359e+06</td>\n",
       "      <td>290318.000000</td>\n",
       "      <td>46686.000000</td>\n",
       "      <td>2.564506e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1837.000000</td>\n",
       "      <td>681.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>2427.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>539.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              is_obf  char_hash_count  char_paren_count  char_brack_count  \\\n",
       "count  204280.000000     2.042800e+05     204280.000000      2.042800e+05   \n",
       "mean        2.019703     6.025353e+01         74.440043      5.056465e+01   \n",
       "std         0.224827     3.665022e+03        576.111577      4.289082e+03   \n",
       "min         1.000000     0.000000e+00          0.000000      0.000000e+00   \n",
       "25%         2.000000     0.000000e+00          6.000000      0.000000e+00   \n",
       "50%         2.000000     4.000000e+00         18.000000      6.000000e+00   \n",
       "75%         2.000000     1.700000e+01         52.000000      2.400000e+01   \n",
       "max         3.000000     1.280462e+06     165592.000000      1.918417e+06   \n",
       "\n",
       "       char_brace_count  char_bkslash_count  char_fwslash_count  \\\n",
       "count     204280.000000       204280.000000        2.042800e+05   \n",
       "mean          94.351963           11.562571        7.086974e+01   \n",
       "std         3400.263950          630.926984        3.748793e+03   \n",
       "min            0.000000            0.000000        0.000000e+00   \n",
       "25%            4.000000            0.000000        0.000000e+00   \n",
       "50%           12.000000            1.000000        2.000000e+00   \n",
       "75%           34.000000            7.000000        8.000000e+00   \n",
       "max       580658.000000       279010.000000        1.104359e+06   \n",
       "\n",
       "       char_dollar_count  char_squote_count  char_dquote_count  ...  \\\n",
       "count      204280.000000      204280.000000       2.042800e+05  ...   \n",
       "mean          112.866277          30.245565       6.672977e+01  ...   \n",
       "std          1789.083560         291.065866       5.685146e+03  ...   \n",
       "min             0.000000           0.000000       0.000000e+00  ...   \n",
       "25%            11.000000           0.000000       4.000000e+00  ...   \n",
       "50%            27.000000           4.000000       1.400000e+01  ...   \n",
       "75%            70.000000          16.000000       4.000000e+01  ...   \n",
       "max        290318.000000       46686.000000       2.564506e+06  ...   \n",
       "\n",
       "       doc_keyword_return_word_count  doc_keyword_static_word_count  \\\n",
       "count                  204280.000000                  204280.000000   \n",
       "mean                        1.277697                       0.077521   \n",
       "std                         8.710415                       2.402772   \n",
       "min                         0.000000                       0.000000   \n",
       "25%                         0.000000                       0.000000   \n",
       "50%                         0.000000                       0.000000   \n",
       "75%                         0.000000                       0.000000   \n",
       "max                      1837.000000                     681.000000   \n",
       "\n",
       "       doc_keyword_switch_word_count  doc_keyword_throw_word_count  \\\n",
       "count                  204280.000000                 204280.000000   \n",
       "mean                        0.206599                      0.432113   \n",
       "std                         1.489893                      6.803669   \n",
       "min                         0.000000                      0.000000   \n",
       "25%                         0.000000                      0.000000   \n",
       "50%                         0.000000                      0.000000   \n",
       "75%                         0.000000                      0.000000   \n",
       "max                       193.000000                   2426.000000   \n",
       "\n",
       "       doc_keyword_trap_word_count  doc_keyword_try_word_count  \\\n",
       "count                204280.000000               204280.000000   \n",
       "mean                      0.016051                    0.526997   \n",
       "std                       0.278518                    6.689347   \n",
       "min                       0.000000                    0.000000   \n",
       "25%                       0.000000                    0.000000   \n",
       "50%                       0.000000                    0.000000   \n",
       "75%                       0.000000                    0.000000   \n",
       "max                      51.000000                 2427.000000   \n",
       "\n",
       "       doc_keyword_until_word_count  doc_keyword_using_word_count  \\\n",
       "count                 204280.000000                 204280.000000   \n",
       "mean                       0.041428                      0.272983   \n",
       "std                        0.461789                      2.055392   \n",
       "min                        0.000000                      0.000000   \n",
       "25%                        0.000000                      0.000000   \n",
       "50%                        0.000000                      0.000000   \n",
       "75%                        0.000000                      0.000000   \n",
       "max                       42.000000                    539.000000   \n",
       "\n",
       "       doc_keyword_var_word_count  doc_keyword_while_word_count  \n",
       "count               204280.000000                 204280.000000  \n",
       "mean                     0.256893                      0.153363  \n",
       "std                      2.049805                      0.928370  \n",
       "min                      0.000000                      0.000000  \n",
       "25%                      0.000000                      0.000000  \n",
       "50%                      0.000000                      0.000000  \n",
       "75%                      0.000000                      0.000000  \n",
       "max                    191.000000                     88.000000  \n",
       "\n",
       "[8 rows x 79 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the summary of our pandas DataFrame. Ignore the 'is_obf' column. That is the target column. \n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay there is a lot of scientific notation present. Huge numbers and small. One way we can try to normalize the data is to make the counts of features a ratio of number of chars are in a documents. This method may help us mitigate cases where very very large scripts can skew the data.\n",
    "\n",
    "The function below should modify the data frame to transform all features we want transformed excluding the columns that would otherwise have no effect because of num of chars in a file. It can be skipped by setter the 'data_to_percent' variable to False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to modify the data to turn feature values from count to ratio\n",
    "data_to_percent = True\n",
    "\n",
    "def transform_feature_count_to_percent(dataframe):\n",
    "    \"\"\"Function to turn certain features from count to percentage (ratio) values. It\n",
    "    does this be deviding the feature value by the document char count. Document char count\n",
    "    is also a feature within the dataset and will be ignored along with some other features.\n",
    "    This transformation will help try to mitigate the affects of scaling, because as the script \n",
    "    gets larger than then char count would go up too. \n",
    "\n",
    "    Returns transformed Pandas DataFrame \n",
    "    \"\"\"\n",
    "    working_data = dataframe  # Create DF to return\n",
    "    char_count_col_name = 'doc_char_count'\n",
    "\n",
    "    # List of features that would not likely be susceptible to scaling issues\n",
    "    non_sus_feat_lst = ['doc_char_count', 'doc_avg_line_len', 'doc_min_line_len',\n",
    "                        'doc_line_count', 'doc_mcomment_count', 'doc_entropy', 'is_obf']\n",
    "\n",
    "    # Start looping through the working data and change the values \n",
    "    for row in list(working_data.index.values):\n",
    "        for col_name in working_data.columns.tolist():\n",
    "            # Skip columns that we don't want to modify\n",
    "            if col_name not in non_sus_feat_lst:\n",
    "                working_data.at[row, col_name] = (\n",
    "                    int(working_data.at[row, col_name]) / int(working_data.at[row, char_count_col_name])\n",
    "                    )\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return(working_data)\n",
    "\n",
    "# Transform if data_to_percent is true\n",
    "if data_to_percent:\n",
    "    delta_data_df = transform_feature_count_to_percent(delta_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's check if it made a difference. Print out the Description of the DataFrame if the transformation was performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs if the transformation was performed\n",
    "if data_to_percent:\n",
    "    delta_data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A principle component analysis will be used for this data. We are treating it as an unsupervised approach. Lables were added later to the data to hopefully discover some clustering. We will also use original data along side the delta data for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a copy of and drop that target column so it doesn't intefere with the PCA \n",
    "target_df = pd.DataFrame(data_df['is_obf']) # They are both the same so I only need one. \n",
    "# Drop column from DF's \n",
    "data_df = data_df.drop(columns=['is_obf'])\n",
    "delta_data_df = delta_data_df.drop(columns=['is_obf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple function to determin the best n components for PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal n_components for the delta data is 2\n",
      "Optimal n_components for the original data is 4\n"
     ]
    }
   ],
   "source": [
    "# Creating a function to find the optimal n_components for PCA \n",
    "def find_opt_n_components(data_set):\n",
    "\t'''Find the number of components that can explain the most data.'''\n",
    "\tset_percentage = .98 # Percentage of data explained\n",
    "\n",
    "\t# The range is the number of components to test\n",
    "\tfor comp in range(2, data_set.shape[1]):\n",
    "\t\tpca = PCA(n_components = comp, random_state=42)\n",
    "\t\tpca.fit(data_set)\n",
    "\t\tcomp_check = pca.explained_variance_ratio_\n",
    "\t\tfinal_comp = comp\n",
    "\t\t\n",
    "\t\tif comp_check.sum() >= set_percentage:\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn(final_comp)\n",
    "\n",
    "opt_n_delta_data = find_opt_n_components(delta_data_df)\n",
    "print(\"Optimal n_components for the delta data is {}\".format(opt_n_delta_data))\n",
    "\n",
    "opt_n_data = find_opt_n_components(data_df)\n",
    "print(\"Optimal n_components for the original data is {}\".format(opt_n_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create some PCA with our n_components. We will make one for the delta and the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Delta] Explained variation per principal component: [0.97380168 0.02611931]\n",
      "\n",
      "\n",
      "[Original] Explained variation per principal component: [0.82619755 0.10880906 0.04209118 0.00652515]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PCA for the delta \n",
    "delta_pca = PCA(n_components=opt_n_delta_data)\n",
    "x_delta_pca = delta_pca.fit(delta_data_df).transform(delta_data_df) # Fit and transform\n",
    "print(\"\\n[Delta] Explained variation per principal component: {}\\n\".format(\n",
    "\tdelta_pca.explained_variance_ratio_))\n",
    "\n",
    "# PCA for the Original Data\n",
    "orig_pca = PCA(n_components=opt_n_data)\n",
    "x_orig_pca = orig_pca.fit(data_df).transform(data_df) # Fit and transform\n",
    "print(\"\\n[Original] Explained variation per principal component: {}\\n\".format(\n",
    "\torig_pca.explained_variance_ratio_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2cebe7ecc4cd892ef2bc456d3ad0bac2cbfd1b5b6071ac27cc635c30a2ed3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
