{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Lab 1\n",
    "Group 2 \n",
    "Members: Josh Mitchell, Adam Alidra, Ryan Herrin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.\n",
    "- Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "- Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "- Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data \n",
    "\n",
    "# NOTE: Data imported has been generated from the first lab and turned into two data sets \n",
    "# + One data set is the raw data minus columns that had no values in them \n",
    "# + The other data set is data that had a transformation applied to it \n",
    "raw_data_location = '../dataset/raw_data.csv'\n",
    "transformed_data_location = '../dataset/transformed_data.csv'\n",
    "\n",
    "raw_data_df = pd.read_csv(raw_data_location)\n",
    "tf_data_df = pd.read_csv(transformed_data_location) # tf being shorthand for transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model and SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into 80/20 Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common random seed value and sample test percentage size \n",
    "seed_value = 8675309 # if you know you know\n",
    "test_sample_size = .2 # percentage \n",
    "\n",
    "# Split the data into a 80/20 Train/Testing for raw and trasnformed data \n",
    "# Using the split method from sklearn \n",
    "raw_train, raw_test = train_test_split(raw_data_df, test_size=test_sample_size, random_state=seed_value)\n",
    "tf_train, tf_test = train_test_split(tf_data_df, test_size=test_sample_size, random_state=seed_value)\n",
    "\n",
    "# Create Reusable Numpy arrays from the raw and transformed data to avoid using loc and iloc to seperate\n",
    "# the contents of the dataframes when training/testing\n",
    "\n",
    "# Raw Data ---\n",
    "raw_train_features = raw_train.iloc[:, 1:].to_numpy() # features to train without 'is_obf'\n",
    "raw_train_labels = raw_train.loc[:, 'is_obf'].to_numpy() # only 'is_obf'\n",
    "raw_test_features = raw_test.iloc[:, 1:].to_numpy() # features to create predictions\n",
    "raw_test_labels = raw_test.loc[:, 'is_obf'].to_numpy() # only 'is_obf' to test accuracy and get matrix with\n",
    "\n",
    "# Transformed Data ---\n",
    "tf_train_features = tf_train.iloc[:, 1:].to_numpy() # features to train without 'is_obf'\n",
    "tf_train_labels = tf_train.loc[:, 'is_obf'].to_numpy() # only 'is_obf'\n",
    "tf_test_features = tf_test.iloc[:, 1:].to_numpy() # features to create predictions\n",
    "tf_test_labels = tf_test.loc[:, 'is_obf'].to_numpy() # only 'is_obf' to test accuracy and get matrix with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression() Notes:\n",
    "- @penalty: L1 = Lasso Regression, L2 = Ridge Regression \"Squared magnitude\"\n",
    "- @C: Float value. \"Smaller values specify stronger regularzation\"\n",
    "- @class_weight: Weights associated with classes in the form {class_label: weight}\n",
    "- @max_iter: default was 100, but 150 was needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9629985583853916\n",
      "Confusion Matrix\n",
      " [[ 614   26]\n",
      " [  51 1390]]\n",
      "\n",
      "Feature Weights\n",
      "---------------------\n",
      "char_bktick_count weight:  -0.03164989108600169\n",
      "char_plus_count weight:  -0.008722062783377765\n",
      "doc_char_count weight:  -0.008256577513094096\n",
      "doc_line_count weight:  -0.0028783623161697212\n",
      "doc_avg_line_len weight:  -0.0024316392065223797\n",
      "char_excl_count weight:  -0.002115552972305724\n",
      "char_brace_count weight:  -0.001976555290790541\n",
      "char_amp_count weight:  -0.0015051750854944276\n",
      "char_comma_count weight:  -0.001253554911067075\n",
      "char_carrot_count weight:  -0.00043275582416423577\n",
      "doc_max_line_len weight:  -0.00030609546000275436\n",
      "doc_dtype_float_word_count weight:  9.4270164154653e-10\n",
      "doc_dtype_double_word_count weight:  7.55880428384928e-09\n",
      "doc_dtype_decimal_word_count weight:  2.295081308796399e-08\n",
      "doc_keyword_trap_word_count weight:  9.858128911420693e-08\n",
      "doc_dtype_char_word_count weight:  1.3203843337635205e-07\n",
      "doc_keyword_dynamicparam_word_count weight:  3.0094425990642196e-07\n",
      "doc_dtype_long_word_count weight:  3.329414983165701e-07\n",
      "doc_keyword_until_word_count weight:  1.0841575918003222e-06\n",
      "doc_dtype_int_word_count weight:  2.4696486330114995e-06\n",
      "doc_keyword_hidden_word_count weight:  2.5619419693832812e-06\n",
      "doc_dtype_bool_word_count weight:  2.6678833984211825e-06\n",
      "doc_keyword_static_word_count weight:  5.076993395840346e-06\n",
      "doc_dtype_single_word_count weight:  5.239893539000094e-06\n",
      "doc_keyword_continue_word_count weight:  6.986110168532796e-06\n",
      "doc_keyword_filter_word_count weight:  7.003174028297435e-06\n",
      "doc_keyword_while_word_count weight:  7.917776921652028e-06\n",
      "doc_keyword_finally_word_count weight:  8.074230416300463e-06\n",
      "doc_keyword_define_word_count weight:  8.439427245492028e-06\n",
      "doc_keyword_enum_word_count weight:  1.0143961869178212e-05\n",
      "doc_keyword_elseif_word_count weight:  1.03179161235594e-05\n",
      "doc_keyword_data_word_count weight:  1.3572235471279577e-05\n",
      "doc_keyword_exit_word_count weight:  1.5103907682959559e-05\n",
      "doc_keyword_switch_word_count weight:  1.5723984830578008e-05\n",
      "doc_keyword_break_word_count weight:  1.9787515416809447e-05\n",
      "doc_keyword_class_word_count weight:  2.0597853812752744e-05\n",
      "doc_keyword_begin_word_count weight:  2.0865634397852184e-05\n",
      "doc_keyword_end_word_count weight:  2.2870262809286306e-05\n",
      "doc_keyword_var_word_count weight:  2.6395593144891738e-05\n",
      "doc_keyword_using_word_count weight:  2.8903356292348097e-05\n",
      "doc_keyword_process_word_count weight:  5.270103781501283e-05\n",
      "doc_keyword_catch_word_count weight:  5.978208708082045e-05\n",
      "doc_keyword_try_word_count weight:  7.168723375794404e-05\n",
      "doc_keyword_throw_word_count weight:  7.368544294435583e-05\n",
      "doc_keyword_do_word_count weight:  7.756518998086953e-05\n",
      "doc_keyword_else_word_count weight:  0.00010298511103008243\n",
      "doc_keyword_foreach_word_count weight:  0.0001774521782401998\n",
      "doc_keyword_from_word_count weight:  0.0001872059369851939\n",
      "char_percent_count weight:  0.0002685261710859404\n",
      "doc_keyword_return_word_count weight:  0.0003411523842380313\n",
      "doc_keyword_function_word_count weight:  0.00037656238981084836\n",
      "doc_keyword_param_word_count weight:  0.0003992845186008976\n",
      "doc_mcomment_count weight:  0.0004330366108881537\n",
      "doc_keyword_if_word_count weight:  0.0004558756273106133\n",
      "doc_keyword_in_word_count weight:  0.00047487090454703017\n",
      "char_squote_count weight:  0.0009053340557880586\n",
      "char_bkslash_count weight:  0.006657932110795562\n",
      "doc_entropy weight:  0.00786336407711396\n",
      "char_brack_count weight:  0.009079760241217575\n",
      "char_glthan_count weight:  0.010498273979552062\n",
      "char_pipe_count weight:  0.011858547293649954\n",
      "char_star_count weight:  0.014745758674103975\n",
      "char_ucase_count weight:  0.01618753222337052\n",
      "char_hash_count weight:  0.016579303425402675\n",
      "char_space_count weight:  0.016631475209148656\n",
      "char_lcase_count weight:  0.016683792585727823\n",
      "char_period_count weight:  0.016817781167681144\n",
      "char_eq_count weight:  0.017732975715057515\n",
      "doc_min_line_len weight:  0.0183684666428022\n",
      "char_colon_count weight:  0.019101551461982277\n",
      "char_num_count weight:  0.019758343366152186\n",
      "char_minus_count weight:  0.01999124021652476\n",
      "char_uscore_count weight:  0.021329341415687757\n",
      "char_fwslash_count weight:  0.0222465107056378\n",
      "char_dollar_count weight:  0.02298288230346981\n",
      "char_scolon_count weight:  0.023077229461197477\n",
      "char_paren_count weight:  0.025185460799963155\n",
      "char_dquote_count weight:  0.027730129707545757\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Raw Data #\n",
    "\n",
    "# Create Logistic Regression Object \n",
    "log_reg_raw = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear', max_iter=150)\n",
    "\n",
    "# Train model with raw data input\n",
    "log_reg_raw.fit(raw_train_features, raw_train_labels)\n",
    "\n",
    "# Create predictions from model\n",
    "raw_log_reg_pred = log_reg_raw.predict(raw_test_features)\n",
    "\n",
    "# Get accuracy \n",
    "raw_log_reg_accuracy = mt.accuracy_score(\n",
    "    raw_test_labels, # Original lables (classifications)\n",
    "    raw_log_reg_pred # Predicted values\n",
    ")\n",
    "\n",
    "# Get confusion Matrix\n",
    "raw_log_reg_conf = mt.confusion_matrix(raw_test_labels, raw_log_reg_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Accuracy: \", raw_log_reg_accuracy)\n",
    "print(\"Confusion Matrix\\n\", raw_log_reg_conf)\n",
    "\n",
    "# Get the weights for each variable \n",
    "# sort these attributes and spit them out\n",
    "raw_logreg_vars = zip(log_reg_raw.coef_.T, raw_train.iloc[:, 1:])\n",
    "raw_logreg_vars = sorted(raw_logreg_vars)\n",
    "# Display Weights \n",
    "print('\\nFeature Weights\\n---------------------')\n",
    "for coef, name in raw_logreg_vars:\n",
    "    print(name, 'weight: ', coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9553099471407976\n",
      "Confusion Matrix\n",
      " [[ 609   31]\n",
      " [  62 1379]]\n",
      "\n",
      "Feature Weights\n",
      "---------------------\n",
      "char_bktick_count weight:  -5.180133470250689\n",
      "char_amp_count weight:  -1.1470676328264289\n",
      "char_plus_count weight:  -1.0128240760699014\n",
      "char_squote_count weight:  -0.8882056277368777\n",
      "char_brace_count weight:  -0.8706667191172535\n",
      "char_period_count weight:  -0.8597392512050249\n",
      "char_excl_count weight:  -0.8284191807347966\n",
      "char_percent_count weight:  -0.7919766195796991\n",
      "char_brack_count weight:  -0.694828883973486\n",
      "char_bkslash_count weight:  -0.6707975608886608\n",
      "char_glthan_count weight:  -0.20695229026110692\n",
      "char_carrot_count weight:  -0.18401555580292345\n",
      "char_comma_count weight:  -0.11338066467028637\n",
      "char_paren_count weight:  0.029193321738764873\n",
      "char_num_count weight:  0.061869634270520155\n",
      "char_space_count weight:  0.10669898919361266\n",
      "char_ucase_count weight:  0.11831948127704567\n",
      "char_hash_count weight:  0.18352265321574418\n",
      "char_lcase_count weight:  0.1906051739445954\n",
      "char_scolon_count weight:  0.20130384324533196\n",
      "char_star_count weight:  0.2145836200571288\n",
      "char_eq_count weight:  0.2536968846582233\n",
      "char_uscore_count weight:  0.2736852182194147\n",
      "char_fwslash_count weight:  0.27791445416998534\n",
      "char_minus_count weight:  0.5311185233738093\n",
      "char_colon_count weight:  0.9156791171455849\n",
      "char_dollar_count weight:  1.0967848203627797\n",
      "char_dquote_count weight:  1.3291440847537623\n",
      "char_pipe_count weight:  1.7530035017515853\n",
      "doc_keyword_sum weight:  3.7395740276272087\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Transformed Data \n",
    "\n",
    "# Create Logistic Regression Object \n",
    "log_reg_tf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear', max_iter=150)\n",
    "\n",
    "# Train model with raw data input\n",
    "log_reg_tf.fit(tf_train_features, tf_train_labels)\n",
    "\n",
    "# Create predictions from model\n",
    "tf_log_reg_pred = log_reg_tf.predict(tf_test_features)\n",
    "\n",
    "# Get accuracy \n",
    "tf_log_reg_accuracy = mt.accuracy_score(\n",
    "    tf_test_labels, # Original lables (classifications)\n",
    "    tf_log_reg_pred # Predicted values\n",
    ")\n",
    "\n",
    "# Get confusion Matrix\n",
    "tf_log_reg_conf = mt.confusion_matrix(tf_test_labels, tf_log_reg_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Accuracy: \", tf_log_reg_accuracy)\n",
    "print(\"Confusion Matrix\\n\", tf_log_reg_conf)\n",
    "\n",
    "# Get the weights for each variable \n",
    "# sort these attributes and spit them out\n",
    "tf_logreg_vars = zip(log_reg_tf.coef_.T, tf_train.iloc[:, 1:])\n",
    "tf_logreg_vars = sorted(tf_logreg_vars)\n",
    "# Display Weights \n",
    "print('\\nFeature Weights\\n---------------------')\n",
    "for coef, name in tf_logreg_vars:\n",
    "    print(name, 'weight: ', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model for Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model for Transformed Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2cebe7ecc4cd892ef2bc456d3ad0bac2cbfd1b5b6071ac27cc635c30a2ed3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
