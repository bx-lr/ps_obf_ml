{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Lab 1\n",
    "Group 2 \n",
    "Members: Josh Mitchell, Adam Alidra, Ryan Herrin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.\n",
    "- Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "- Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "- Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data \n",
    "\n",
    "# NOTE: Data imported has been generated from the first lab and turned into two data sets \n",
    "# + One data set is the raw data minus columns that had no values in them \n",
    "# + The other data set is data that had a transformation applied to it \n",
    "raw_data_location = '../dataset/raw_data.csv'\n",
    "transformed_data_location = '../dataset/transformed_data.csv'\n",
    "\n",
    "raw_data_df = pd.read_csv(raw_data_location)\n",
    "tf_data_df = pd.read_csv(transformed_data_location) # tf being shorthand for transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model and SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into 80/20 Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common random seed value and sample test percentage size \n",
    "seed_value = 8675309 # if you know you know\n",
    "test_sample_size = .2 # percentage \n",
    "\n",
    "# Split the data into a 80/20 Train/Testing for raw and trasnformed data \n",
    "# Using the split method from sklearn \n",
    "raw_train, raw_test = train_test_split(raw_data_df, test_size=test_sample_size, random_state=seed_value)\n",
    "tf_train, tf_test = train_test_split(tf_data_df, test_size=test_sample_size, random_state=seed_value)\n",
    "\n",
    "# Create Reusable Numpy arrays from the raw and transformed data to avoid using loc and iloc to seperate\n",
    "# the contents of the dataframes when training/testing\n",
    "\n",
    "# Raw Data ---\n",
    "raw_train_features = raw_train.iloc[:, 1:].to_numpy() # features to train without 'is_obf'\n",
    "raw_train_labels = raw_train.loc[:, 'is_obf'].to_numpy() # only 'is_obf'\n",
    "raw_test_features = raw_test.iloc[:, 1:].to_numpy() # features to create predictions\n",
    "raw_test_labels = raw_test.loc[:, 'is_obf'].to_numpy() # only 'is_obf' to test accuracy and get matrix with\n",
    "\n",
    "# Transformed Data ---\n",
    "tf_train_features = tf_train.iloc[:, 1:].to_numpy() # features to train without 'is_obf'\n",
    "tf_train_labels = tf_train.loc[:, 'is_obf'].to_numpy() # only 'is_obf'\n",
    "tf_test_features = tf_test.iloc[:, 1:].to_numpy() # features to create predictions\n",
    "tf_test_labels = tf_test.loc[:, 'is_obf'].to_numpy() # only 'is_obf' to test accuracy and get matrix with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression() Notes:\n",
    "- @penalty: L1 = Lasso Regression, L2 = Ridge Regression \"Squared magnitude\"\n",
    "- @C: Float value. \"Smaller values specify stronger regularzation\"\n",
    "- @class_weight: Weights associated with classes in the form {class_label: weight}\n",
    "- @max_iter: default was 100, but 150 was needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model for Raw Data #\n",
    "\n",
    "# Create Logistic Regression Object \n",
    "log_reg_raw = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear', max_iter=150)\n",
    "\n",
    "# Train model with raw data input\n",
    "log_reg_raw.fit(raw_train_features, raw_train_labels)\n",
    "\n",
    "# Create predictions from model\n",
    "raw_log_reg_pred = log_reg_raw.predict(raw_test_features)\n",
    "\n",
    "# Get accuracy \n",
    "raw_log_reg_accuracy = mt.accuracy_score(\n",
    "    raw_test_labels, # Original lables (classifications)\n",
    "    raw_log_reg_pred # Predicted values\n",
    ")\n",
    "\n",
    "# Get confusion Matrix\n",
    "raw_log_reg_conf = mt.confusion_matrix(raw_test_labels, raw_log_reg_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Accuracy: \", raw_log_reg_accuracy)\n",
    "print(\"Confusion Matrix\\n\", raw_log_reg_conf)\n",
    "\n",
    "# Get the weights for each variable \n",
    "# sort these attributes and spit them out\n",
    "raw_logreg_vars = zip(log_reg_raw.coef_.T, raw_train.iloc[:, 1:])\n",
    "raw_logreg_vars = sorted(raw_logreg_vars)\n",
    "# Display Weights \n",
    "print('\\nFeature Weights\\n---------------------')\n",
    "for coef, name in raw_logreg_vars:\n",
    "    print(name, 'weight: ', coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model for Transformed Data \n",
    "\n",
    "# Create Logistic Regression Object \n",
    "log_reg_tf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear', max_iter=150)\n",
    "\n",
    "# Train model with raw data input\n",
    "log_reg_tf.fit(tf_train_features, tf_train_labels)\n",
    "\n",
    "# Create predictions from model\n",
    "tf_log_reg_pred = log_reg_tf.predict(tf_test_features)\n",
    "\n",
    "# Get accuracy \n",
    "tf_log_reg_accuracy = mt.accuracy_score(\n",
    "    tf_test_labels, # Original lables (classifications)\n",
    "    tf_log_reg_pred # Predicted values\n",
    ")\n",
    "\n",
    "# Get confusion Matrix\n",
    "tf_log_reg_conf = mt.confusion_matrix(tf_test_labels, tf_log_reg_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Accuracy: \", tf_log_reg_accuracy)\n",
    "print(\"Confusion Matrix\\n\", tf_log_reg_conf)\n",
    "\n",
    "# Get the weights for each variable \n",
    "# sort these attributes and spit them out\n",
    "tf_logreg_vars = zip(log_reg_tf.coef_.T, tf_train.iloc[:, 1:])\n",
    "tf_logreg_vars = sorted(tf_logreg_vars)\n",
    "# Display Weights \n",
    "print('\\nFeature Weights\\n---------------------')\n",
    "for coef, name in tf_logreg_vars:\n",
    "    print(name, 'weight: ', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model for Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model for Transformed Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2cebe7ecc4cd892ef2bc456d3ad0bac2cbfd1b5b6071ac27cc635c30a2ed3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
