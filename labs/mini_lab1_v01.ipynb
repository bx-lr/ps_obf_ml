{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Lab 1\n",
    "Group 2 \n",
    "Members: Josh Mitchell, Adam Alidra, Ryan Herrin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "- Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.\n",
    "- Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "- Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "- Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data \n",
    "\n",
    "# NOTE: Data imported has been generated from the first lab and turned into two data sets \n",
    "# + One data set is the raw data minus columns that had no values in them \n",
    "# + The other data set is data that had a transformation applied to it \n",
    "raw_data_location = '../dataset/all_with_keyword_sum.csv'\n",
    "transformed_data_location = '../dataset/transformed_data.csv'\n",
    "delta_pca_location = '../dataset/delta_pca_data.csv'\n",
    "\n",
    "raw_data_df = pd.read_csv(raw_data_location)\n",
    "tf_data_df = pd.read_csv(transformed_data_location) # tf being shorthand for transformed\n",
    "delta_pca = pd.read_csv(delta_pca_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to standardize how we modify incoming new csv datasets \n",
    "def is_obf_pipline(dataframe):\n",
    "    pd_df = dataframe\n",
    "    unused_columns = ['vt_harmless', 'vt_undetected', 'vt_malicious', 'vt_suspicious',\n",
    "\t\t\t\t      'avclass_name', 'obf_name', 'sha1', 'fpath']\n",
    "    # Remove Duplicates\n",
    "    pd_df.drop_duplicates('sha1', inplace=True)\n",
    "\n",
    "    # Check to see if duplicate header is present. Remove if there is \n",
    "    if pd_df.at[0, 'sha1'] == 'sha1':\n",
    "        pd_df = pd_df.iloc[1:] \n",
    "\n",
    "\t# Remove unused columns \n",
    "    pd_df = pd_df.drop(columns=unused_columns)\n",
    "    \n",
    "    # Keep only the rows that are labled 1 or 3 in the \"is_obf\" column\n",
    "    pd_df['is_obf'] = pd.to_numeric(pd_df['is_obf'], downcast='integer')\n",
    "    pd_df.drop(pd_df.loc[pd_df['is_obf']==2].index, inplace=True)\n",
    "\n",
    "    return(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform common transformation on raw data\n",
    "raw_data_df = is_obf_pipline(raw_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model and SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common random seed value and sample test percentage size \n",
    "seed_value = 8675309 # if you know you know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80/20 Train/Test Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data(dataframe, label_name, seed):\n",
    "    '''Split the data from the dataframe into 80/20 test and returns the training data, \n",
    "    testing data, training/test features, train/test lables'''\n",
    "    df_train, df_test = train_test_split(dataframe, test_size=.2, random_state=seed)\n",
    "    df_train_features = df_train[df_train.columns[df_train.columns != label_name]].to_numpy() \n",
    "    df_test_features = df_test[df_test.columns[df_test.columns != label_name]].to_numpy()\n",
    "    df_train_labels = df_train[df_train.columns[df_train.columns == label_name]].to_numpy().ravel()\n",
    "    df_test_labels = df_test[df_test.columns[df_test.columns == label_name]].to_numpy().ravel()  \n",
    "\n",
    "    return(df_train, df_test, df_train_features, df_test_features, df_train_labels, df_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression() Notes:\n",
    "- @penalty: L1 = Lasso Regression, L2 = Ridge Regression \"Squared magnitude\"\n",
    "- @C: Float value. \"Smaller values specify stronger regularzation\"\n",
    "- @class_weight: Weights associated with classes in the form {class_label: weight}\n",
    "- @max_iter: default was 100, but 150 was needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generated logistic regression model\n",
    "def  create_logistic_model(dataframe, label, seed=None):\n",
    "    '''Function to create a linear regression model. When ran it will display the accuracy and confusion matrix of the model.\n",
    "    @Params:\n",
    "    - dataframe (pandasDataframe) : Dataframe Object\n",
    "    - label (str) : name of the column that defines the label \n",
    "    - seed (optional) (int) : integer value for a seed\n",
    "    @Returns: fitted LogisticRegression()\n",
    "    '''\n",
    "    # Create logistic regression model\n",
    "    logreg_model = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear', max_iter=150) \n",
    "    # Data split\n",
    "    df_train, df_test, df_train_features, df_test_features, df_train_labels, df_test_labels = _split_data(\n",
    "        dataframe, label, seed\n",
    "    )\n",
    "    # Fit the model\n",
    "    logreg_model.fit(df_train_features, df_train_labels)\n",
    "    # Test out the model and create predictions \n",
    "    logreg_pred = logreg_model.predict(df_test_features)\n",
    "    # get accuracy \n",
    "    logreg_accuracy = mt.accuracy_score(df_test_labels, logreg_pred)\n",
    "    # Confusion Matrix \n",
    "    logreg_conf = mt.confusion_matrix(df_test_labels, logreg_pred)\n",
    "\n",
    "    # Display results \n",
    "    print(\"Accuracy: \", logreg_accuracy)\n",
    "    print(\"Confusion Matrix\\n\", logreg_conf)  \n",
    "\n",
    "    return(logreg_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw Data Model and Results')\n",
    "raw_data_logreg_model = create_logistic_model(raw_data_df, 'is_obf', seed_value)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Transformed Model and Results')\n",
    "tf_data_logreg_model = create_logistic_model(tf_data_df, 'is_obf', seed_value)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Delta PCA Model and Results')\n",
    "delta_pca_logreg_model = create_logistic_model(delta_pca, 'is_obf', seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Weight Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logreg_weights(dataframe, log_model, label_name, title=None, v=False):\n",
    "    '''Prints the weights of each feature in the model and displays a graphic. Returns nothing. \n",
    "    @Params:\n",
    "    - dataframe (PandasDataframe) : Dataframe used when creating the model\n",
    "    - log_model (LogisticRegression) : Model returned from the create_logistic_model() function\n",
    "    - label_name (string) : name of the lable column used to created the model\n",
    "    - title (string) : Title of the bar graph\n",
    "    - v (bool) : If true, prints the wieghts. If False, only prints chart\n",
    "    '''\n",
    "    # Create dataframe to send to bar grpah \n",
    "    logreg_tmp_df = dataframe[dataframe.columns[dataframe.columns != label_name]]\n",
    "    logreg_wghts = pd.DataFrame(log_model.coef_[0], columns=['value'], index=logreg_tmp_df.columns)\n",
    "\n",
    "    # display those that have a weight greater than .001\n",
    "    logreg_wghts = logreg_wghts.loc[(logreg_wghts[\"value\"] >= .001) | (logreg_wghts[\"value\"] <= -.001)]\n",
    "\n",
    "    # Plot / Display\n",
    "    if title == None: \n",
    "        logreg_wghts.plot(kind='bar', title='Feature Weights')\n",
    "    else:\n",
    "        logreg_wghts.plot(kind='bar', title=title)\n",
    "    plt.show()\n",
    "\n",
    "    if v == True:\n",
    "        # Display text Versions \n",
    "        # Get the weights for each variable \n",
    "        logreg_vars = zip(log_model.coef_.T, logreg_tmp_df)\n",
    "        logreg_vars = sorted(logreg_vars)\n",
    "        # Display Weights \n",
    "        print('\\nFeature Weights\\n---------------------')\n",
    "        for coef, name in logreg_vars:\n",
    "            print(name, 'weight: ', coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Weights for Raw Data\n",
    "get_logreg_weights(raw_data_df, raw_data_logreg_model, 'is_obf', title='Raw Data Feature Weights', v=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Weights for Transformed Data\n",
    "get_logreg_weights(tf_data_df, tf_data_logreg_model, 'is_obf', title='Transformed Feature Weights', v=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Weights for Transformed Data\n",
    "get_logreg_weights(delta_pca, delta_pca_logreg_model, 'is_obf', title='Delta PCA Feature Weights', v=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for SVM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for visualiztion of SVM models \n",
    "'''\n",
    "Referenced from:\n",
    "https://chrisalbon.com/code/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/\n",
    "'''\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=cmap(idx),\n",
    "                    marker=markers[idx], label=cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_svm_model(dataframe, label, seed, svm_kernal='rbf', n_folds=5):\n",
    "    '''Create SVM model. When ran it will display the accuracy and confusion matrix of the model. Returns SVM Model.\n",
    "    @Params:\n",
    "    - dataframe (pandasDataFrame) : Dataframe to run \n",
    "    - lable (string) : Name of column that's used as the lable\n",
    "    - seed (optional) (int) : seed to keep random value the same \n",
    "    - svm_kernal : Refer to sklearn.svm for options. {linear, poly, rbf, sigmoid, precomputed}\n",
    "    - n_folds (int) : number of k-folds that will be passed to the param selection pipeline. Default = 5\n",
    "    @Returns: SVM Model\n",
    "    '''\n",
    "    # Data split\n",
    "    df_train, df_test, df_train_features, df_test_features, df_train_labels, df_test_labels = _split_data(\n",
    "        dataframe, label, seed\n",
    "    )\n",
    "\n",
    "    def svc_param_selection(X, y, n_folds):\n",
    "        '''SVM Pipeline function to find the best parameters for SVM'''\n",
    "        c = [0.001, 0.01, 0.1, 1, 10] # regularization is inversely proportional to C\n",
    "        gammas = [0.001, 0.01, 0.1, 1] # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "        param_grid = {'C': c, 'gamma' : gammas}\n",
    "        grid_search = GridSearchCV(svm.SVC(kernel=svm_kernal), param_grid, cv=n_folds)\n",
    "        grid_search.fit(X, y)\n",
    "        grid_search.best_params_\n",
    "\n",
    "        return(grid_search.best_params_)\n",
    "\n",
    "    # Get the best parameters for C and gamma using a pipeline\n",
    "    best_params = svc_param_selection(df_train_features, df_train_labels, n_folds)\n",
    "    svm_C = best_params['C']\n",
    "    svm_gamma = best_params['gamma']\n",
    "\n",
    "    print(\"Creating SVM using options ->  C: {}, kernel: {}, gamma: {}\".format(\n",
    "        svm_C, svm_kernal, svm_gamma)\n",
    "        )\n",
    "\n",
    "    # Model creation and testing\n",
    "    svm_model = SVC(C=svm_C, kernel=svm_kernal, gamma=svm_gamma) # Create the SVM model object \n",
    "    svm_model.fit(df_train_features, df_train_labels) # Fit the model\n",
    "    svm_y_hat = svm_model.predict(df_test_features) # Create predictions from the test data features\n",
    "    # Get accuracy \n",
    "    svm_acc = mt.accuracy_score(df_test_labels, svm_y_hat)\n",
    "    # Get confusion matrix \n",
    "    svm_conf = mt.confusion_matrix(df_test_labels, svm_y_hat)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Accuracy: \", svm_acc)\n",
    "    print(\"Confusion Matrix\\n\", svm_conf)\n",
    "\n",
    "    return(svm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed Data SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed data SVM model \n",
    "tf_svm_model = create_svm_model(tf_data_df, 'is_obf', seed_value, svm_kernal='rbf', n_folds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Data SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data SVM model\n",
    "raw_svm_model = create_svm_model(raw_data_df, 'is_obf', seed_value, svm_kernal='rbf', n_folds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta_PCA SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta PCA model \n",
    "delta_pca_svm_model = create_svm_model(delta_pca, 'is_obf', seed_value, svm_kernal='rbf', n_folds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the PCA ony has two features we can actually visualize the SVM using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundaries\n",
    "# NOTE: This chart takes 25+ minutes to run\n",
    "plot_decision_regions(\n",
    "    delta_pca.loc[:, ('PC1', 'PC2')].to_numpy(), \n",
    "    delta_pca.loc[:, ('is_obf')].to_numpy(), \n",
    "    classifier=delta_pca_svm_model\n",
    "    )\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2cebe7ecc4cd892ef2bc456d3ad0bac2cbfd1b5b6071ac27cc635c30a2ed3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
